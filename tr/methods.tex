\section{Methods}

\subsection{Exponential-family}

The exponential-family distributions for standard GLMMs are defined as
\begin{align*}
  y \sim \exp\{(y\theta - f_b(\theta))/f_a(\phi) + f_c(y, \phi)\}
\end{align*}
with the canonical $\theta$ and dispersion $\phi$ parameters.
The variable $y$ is naturally the outcome modelled by a distribution that
emerges from the definition of the $f_{\cdot}(\cdot)$ functions and of the two
parameters. As an example, the Bernoulli distribution with probability of
success $p$ emerges from the above formulae by setting
\begin{align*}
  \theta = \log\left(\frac{p}{1-p}\right), \phi = 1, f_a(\phi) = \phi,
	f_b(\theta)=\log(1+e^\theta), f_c(y, \phi) = 0
\end{align*}
This class of distributions has the following two properties (McCullagh89):
\begin{align*}
  \mathrm E[y|\theta] &= f'_b(\theta)\\
  \mathrm V[y|\theta] &= f''_b(\theta) f_a(\phi)
\end{align*}

\subsection{Generalised linear mixed models}

It requires first the definition of a twice-differentiable function
$g:\mathcal R \rightarrow \mathcal R$ which will be used to associate a latent
random variable $z$ to the outcome expectation as follows:
\begin{align*}
  g(\mathrm E[y|\theta]) = z
\end{align*}
The above equation makes clear the connection of $z$ with $\theta$ by also
considering Eq. (1).

\section{Instance of the model}

Given a dataset $\data = \{\mathbf y, \mathrm A, \mathrm B\}$ of $n$
individuals together with the observed distribution specification
$\outcome=\{\outcome_i\}_1^n$, where each
$\outcome_i = \{\theta_i, \phi_i, f_{a_i}(\cdot), f_{b_i}(\cdot), f_{c_i}(\cdot, \cdot), g_i(\cdot)\}$
defines an (potentially distinct) exponential-family distribution and link
function, we have
$\mathbf z = \mathrm A\Tr\balpha + \mathrm B\Tr\bbeta + \bepsilon$
multivariate distributed according to
\begin{align*}
  \mathbf z \sim \normal{\mathrm A\Tr \balpha}{\sigma^2_{\beta} \mathrm B\mathrm
	B\Tr + \sigma^2_{\epsilon} \mathrm I}
\end{align*}
Moreover, we have now a connection between the latent space and the observed space
\begin{align*}
  p(y_i | z_i) = \expfam{y_i | \outcome_i, b'(\theta)=g^{-1}(z_i)}
\end{align*}
which fully describes the marginal likelihood
\begin{align}\label{eq:ml}
  p(\mathbf y) = \int \prod_{i=1}^n p(y_i|z_i) \Normal{\mathbf z}{\mathrm A\Tr
	\balpha}{\sigma^2_{\beta} \mathrm B\mathrm B\Tr + \sigma^2_{\epsilon}
	\mathrm I} \mathrm d \mathbf z \tag{5}
\end{align}
used in the maximum likelihood approach for parameter fitting.

\subsection{Interpretation}

\subsubsection{Bernoulli trait}

Let $g(x) = \Phi^{-1}(x)$ be the so-called probit link function and consider the
Bernoulli likelihood as a suitable distribution of the outcome. Conditioning on
the latent variable $z$, we have
\begin{align*}
  p(y|z) = \begin{cases}
  g^{-1}(z) = \Phi(z) & \text{for } y = 1\\
  1-g^{-1}(z) = 1-\Phi(z) & \text{for } y = 0
  \end{cases}
\end{align*}
Let us define a random variable $\tilde y | z = z + e$ in which $z$ is given and
$e$ is standard-Normally distributed random variable such that
\begin{align*}
  \tilde y | z = \begin{cases}
  1 & \text{for } z + e > 0\\
  0 & \text{for } z + e \leq 0
  \end{cases}
\end{align*}
Its p.m.f can thus be written as
\begin{align*}
  p(\tilde y | z) = \begin{cases}
  \Pr{e > -z | z} = \int_{-z}^{+\infty} \Normal{x}{0}{1} \mathrm d x = \Phi(z)  & \text{for } 1\\
  \Pr{e \leq -z | z} = \int_{-\infty}^{-z} \Normal{x}{0}{1} \mathrm d x = 1-\Phi(z) & \text{for } 0
  \end{cases}
\end{align*}
which proves that $p(y|z) = p(\tilde y|z)$. More generally, let $g(x)$ be any
link function (TODO: I might need to assume symmetry here). The definition
\begin{align*}
  p(\tilde y | z) = \begin{cases}
  \Pr{e > -z | z} = g^{-1}(z)  & \text{for } 1\\
  \Pr{e \leq -z | z} = 1-g^{-1}(z) & \text{for } 0
  \end{cases}
\end{align*}
together with equation $\tilde y | z = z + e$ determines the $e$ distribution
via its c.d.f. $g^{-1}(z)$. For example, let $g(\cdot)$ be the so-called logit
link function. We have $e$ distributed according to the zero-mean Logistic
distribution and variance equal to $\pi^2/3$.



A binary trait representing a disease can be defined as

where $e \sim \normal{0}{\sigma_e^2}$ is the source of i.i.d. noise. (talk that
$\sigma_{\epsilon}^2$ cannot be distinguished from $\sigma_e^2$ in the Binary
case)

The value of $z$ can now be understood as the genetic risk for a particular
disease where $\mathbf b$ is a vector of genetic markers (e.g., SNPs) and
$\bbeta$ is the effect-sizes of those markers. The fixed-effect component can
account for non-genetic effects (e.g., smoke, sex, age) whose descriptors are in
$\mathbf a$ and effect-sizes in $\balpha$ to be estimated by the algorithm.

\subsubsection{Binomial trait}

The above model can be naturally extended to the more general one with Binomial
trait $y$. Let us now define a random variable
\begin{align*}
\tilde y | z = \sum_{j=1}^{ntrials} \Ind{z+e_j > 0}
\end{align*}
and the auxiliary one $\tilde y_j | z = \Ind{z+e_j > 0}$ such that $\tilde y = \sum_{j=1}^{ntrials} \tilde y_j$ conditioned on $z$, and $e_j$ are i.i.d. Since the order of the auxiliary variables does not matter, by combination we have
\begin{align*}
p(\tilde y|z) = {ntrials \choose \tilde y} (\Pr{e > -z|z})^{\tilde y} (\Pr{e \leq -z|z})^{ntrials - \tilde y} =
	{ntrials \choose \tilde y} (g^{-1}(z))^{\tilde y} (1-g^{-1}(z))^{ntrials - \tilde y}
\end{align*}
where $e_j \sim \normal{0}{\sigma_e^2}$.

I need to talk about the Binomial support $\{0, 1, \dots, ntrials\}$ versus
$\{0/ntrials, 1/ntrials, \dots, 1\}$.

A draw from the random variable y thus consists in drawing g and ε (only once),
drawing the i.i.d. variables e1, e2, . . . , and er and then counting how many
times the sum μ+g+ε+es is greater than zero. As a concrete example, suppose that
a particular cell have been cloned r times. Those cells would have the same
genetic value g since x is the same for each cell and the additive effects u are
also the same as they are locus-dependent only. Also, those cells would have the
same ε effect due to unaccounted causes that stay constant over the experiment
(e.g., batch effect). Let ys represent whether the cloned cell s has survived
(ys = 1) or not (ys = 0) during a period of time. The number of cells that
survive could be modelled by y, and one might be interested to know how much of
their fate is due to genetics. In this context, it makes sense to model such
responsability as the variance of g over the total variance:

The observable trait variable y conditioned on g and ε is given by a summation
of i.i.d. Bernoulli variables having the probability of succcess equal to Φ((μ +
g + ε)/σe)). Precisely, the likelihood is given by

\subsubsection{Poisson trait}

As far as I know, this is just Binomial with $ntrials \rightarrow +\infty$.
Similar interpretation should happen here.

\subsubsection{Exponential trait}



\subsection{Approximation to probability distribution}

I think we should discuss the methods out there, including the Monte Carlo ones.
We should say that despite its simplicity Monte Carlo methods are slow and there
is no convergence guarantee (and thus human intervention here is required). We
should also discuss deterministic ones like Variational Bayes and explain why we
have chosen EP. This can be done by saying that we need a accurate marginal
likelihood estimation for model comparison and as such EP has been shown to be
empirically unbiased when it comes to moment approximation. As a matter of fact,
EP is based on moment matching afterall. Even thought it might not be the most
desirable method for posterior point-estimation (as it tends to cover multiple
bumps, while VB tends to do the opposite), we really want to know the overall
mass (zeroth moment of a random variable $\mathbf x$ defined by $p(\mathbf x) =
p(\mathbf y, \mathbf x)$). Moreover, we should show that we always have only one
bump as $p(\mathbf x)$ is an unnormalized exponential-family distribution.

\subsection{Expectation Propagation}

EP is a deterministic method for approximating probability distributions in
order to solve complicated integrals as the one define in Eq. \eqref{eq:ml}.
Here it will replace a given exponential-family likelihood $p(y|z)$ by a
non-normalized univariate Normal p.d.f
\begin{align*}
  p(y|z)_{\EP} = \tilde z \Normal{z}{\tilde \mu}{\tilde \sigma^2}
\end{align*}

An instance of our model requires many likelihoods (potentially defined by
heterogenous distributions) together with a multivariate Normal distribution
as a prior, Eq. \eqref{eq:ml}.
For convenience, let us define $\mathbf m = \mathrm A\Tr \balpha$
and $\mathrm K = \sigma^2_{\beta} \mathrm B\mathrm B\Tr + \sigma^2_{\epsilon}
\mathrm I$.
The EP approximation is thus given by
\begin{align*}
p(\mathbf y)_{\text{EP}} = \int \prod_{i=1}^n p(y_i|z_i)_{\EP}
  \Normal{\mathbf z}{\mathbf m}{\mathrm K} d\mathbf z.
\end{align*}

Let
\begin{align*}
    p_{-i}(\mathbf z|\mathbf y) &= \prod_{j\neq i} p(y_j|z_j) p(\mathbf z) / p(\mathbf y)
        \quad\text{ and}\\
    \pcav{i}{\mathbf z|\mathbf y} &= \prod_{j\neq i} \pep{y_j|z_j} p(\mathbf z) / \pep{\mathbf y}
\end{align*}
be the exact and EP cavity probabilities for the $i$-th individual.
Ideally, we would like to minimize the
\begin{align*}
    \KL{p(\mathbf z|\mathbf y)}{\pep{\mathbf z|\mathbf y}} =
      \KL{ p(y_i|z_i) p_{-i}(\mathbf z|\mathbf y) }
        { \pep{y_i|z_i} \pcav{i}{\mathbf z|\mathbf y} }
\end{align*}
divergence over the EP parameters in order to find the best approximation to
$p(\mathbf y) \approx \int \pep{\mathbf y | \mathbf f} p(\mathbf f)$.
Unfortunately, the above problem is intractable in practice.
EP tackles the above problem by instead minimizing the
\begin{align}\label{eq:epkl0}
\KL{ p(y_i|z_i) p_{-i}(\mathbf z | \mathbf y)_{\EP} }{
     p(y_i|z_i)_{\EP} p_{-i}(\mathbf z | \mathbf y)_{\EP} }
\end{align}
divergence for each $i$, repeating the process until convergence.
Proposition \eqref{pr:kleq} shows that the above minimization problem is
equivalent to minimizing an univariate KL divergence
\begin{align}\label{eq:epkl1}
\KL{ p(y_i|z_i) p_{-i}(z_i | \mathbf y)_{\EP} }{
     p(y_i|z_i)_{\EP} p_{-i}(z_i | \mathbf y)_{\EP} },
\end{align}
which is amenable for numerical integration.

The minimum of the KL divergence \eqref{eq:epkl1} is attained when the
right-hand side distribution has its zero-th, first, and second moments equal
to the ones from the left-hand side. (CITATION)
The approximated posterior can now be updated by plug the new
$\tilde z_i, \tilde\mu_i, \tilde\sigma^2_i$ values from the above minimization
into
\begin{gather}
  \pep{\mathbf z| \mathbf y}
    = \frac{\pep{\mathbf y|\mathbf z}p(\mathbf z)}{\pep{\mathbf y}} =
      \Normal{\mathbf z}{\bmu}{\Sigma},\label{eq:pep}\\
        \text{with } \bmu = \Sigma(\mathrm K^{-1} \mathbf m
        + \tilde{\Sigma}^{-1} \tilde{\bmu})
    \text{ and }
    \Sigma = (\mathrm K^{-1} + \tilde{\Sigma}^{-1})^{-1},
      ~~~~ \explain{Lemma \ref{le:gpr}}\nonumber
\end{gather}


 (TODO: refer to Proposition how to compute those).

% p(\mathbf z|\mathbf y)

The minimum of that divergence will provide an update for the values
of $\tilde z_i, \tilde\mu_i, \tilde\sigma^2_i$ which will be then used to update the right-hand sife of
\begin{align*}
\frac{p(\mathbf y, \mathbf z)_{\EP}}{p(\mathbf y)_{\EP}} = \Normal{\mathbf z}{\bmu}{\Sigma}
\end{align*}
where $\bmu = \Sigma(\mathrm K^{-1} \mathbf m + \tilde\Sigma^{-1}\tilde\bmu)$ and $\Sigma=(\mathrm K^{-1} + \tilde\Sigma^{-1})^{-1}$.

The EP marginal log-likelihood is found by integrating $\mathbf z$ out of $p(\mathbf y, \mathbf z)_{\EP}$ which gives us
\begin{align*}
\log p(\mathbf y)_{\EP} = -\frac{1}{2} \log |\mathrm K + \tilde \Sigma| - \frac{1}{2} (\mathbf m - \tilde\bmu)\Tr(\mathrm K + \tilde\Sigma)^{-1}(\mathbf m - \tilde\bmu) +\\
\sum_i \log \hat z_i + \frac{1}{2} \sum_i \log(\tilde \sigma_i^2 + \sigma_{-i}^2) + \sum_i \frac{(\tilde \mu_i - \mu_{-i})^2}{2(\tilde \sigma_i^2 + \sigma_{-i}^2)}
\end{align*}

\subsubsection{Implementation}

An eigen decomposition and a change of variables allow us to write
\begin{align*}
\mathrm K = \sigma_t^2  ((1-\delta)\mathrm Q \mathrm S \mathrm Q^{T} + \delta \mathrm I)
\end{align*}
for $\sigma_b^2=\sigma_t^2(1-\delta)$ and $\sigma_b^2 = \sigma_t \delta$.

The following matrix definitions will help us infer a faster and numerically
safe implementation (correct the bellow equations to account for the fact that I
reparametrized K above)
\begin{align*}
\mathrm A_0 = \sigma_b^{-2} \delta^{-1} \mathrm I \quad \text{if }\delta > 0\\
\mathrm A_1 = (\sigma_b^2 \delta \mathrm I + \tilde\Sigma)^{-1}\\
\mathrm A_1 = \mathrm A_0 (\mathrm A_0 + \tilde{\mathrm T})^{-1} \tilde{\mathrm T} = \tilde{\mathrm T} - \tilde{\mathrm T} (\mathrm A_0 + \tilde{\mathrm T})^{-1} \tilde{\mathrm T} \quad \text{if }\delta > 0\\
\mathrm A_1 = \tilde{\mathrm T} \quad \text{if } \delta = 0\\
\mathrm A_2 = (\mathrm A_0 + \tilde{\mathrm T})^{-1} \quad \text{if } \delta > 0\\
\mathrm B_0 = \mathrm Q^T \mathrm A_0 \mathrm Q + (\sigma_b^2 \mathrm S)^{-1} \quad \text{if } \delta > 0\\
\mathrm B_1 = \mathrm Q^T \mathrm A_1 \mathrm Q + (\sigma_b^2 \mathrm S)^{-1}\\
\mathrm K^{-1} = \mathrm A_0 - \mathrm A_0 \mathrm Q \mathrm B_0^{-1} \mathrm Q^T \mathrm A_0 \quad \text{if } \delta > 0\\
(\mathrm K + \tilde{\Sigma})^{-1} = \mathrm A_1 - \mathrm A_1 \mathrm Q\mathrm B_1^{-1} \mathrm Q^T \mathrm A_1
\end{align*}

\subsubsection{Derivation}

\begin{align*}
\Sigma = \tilde{\Sigma} (\tilde{\Sigma} + \mathrm K)^{-1} \mathrm K = \tilde{\Sigma} (\mathrm A_1 - \mathrm A_1 \mathrm Q \mathrm B^{-1}\mathrm Q\Tr \mathrm A_1) \mathrm K
= \mathrm A_2 \mathrm K - \mathrm A_2 \mathrm Q
          \mathrm B^{-1}\mathrm Q\Tr \mathrm A_1 \mathrm K
\end{align*}

\begin{align*}
\bmu = \tilde{\Sigma} (\tilde{\Sigma} + \mathrm K)^{-1} \mathbf m + \tilde{\Sigma} (\tilde{\Sigma} +
\mathrm K)^{-1} \mathrm K \tilde{\boldsymbol\eta}
= \tilde{\Sigma} (\mathrm A_1 - \mathrm A_1 \mathrm Q \mathrm B^{-1}\mathrm Q\Tr \mathrm A_1) \mathbf m +
\tilde{\Sigma}^{-1} (\mathrm A_1 - \mathrm A_1 \mathrm Q \mathrm B^{-1}\mathrm Q\Tr \mathrm A_1)
          \mathrm K \tilde{\boldsymbol \eta}\\
= \mathrm A_2 \mathbf m - \mathrm A_2 \mathrm Q \mathrm B^{-1}\mathrm Q\Tr \mathrm A_1 \mathbf m
         + \mathrm A_2 \mathrm K \tilde{\boldsymbol \eta} - \mathrm A_2 \mathrm Q \mathrm B^{-1}\mathrm Q\Tr
           \mathrm A_1 \mathrm K \tilde{\boldsymbol \eta}
\end{align*}


\begin{align*}
\Sigma = (\mathrm K^{-1} + \tilde{\Sigma}^{-1})^{-1}\\
\bmu = \Sigma (\mathrm K^{-1} \mathbf m + \tilde{\boldsymbol\eta})
\end{align*}

\subsection{Numerical integration}

We want to compute
\begin{align*}
\int_{-\infty}^{+\infty} z^m \expfam{y |\outcome, g(b'(\theta))=z} \Normal{z}{\mu}{\sigma^2}\mathrm dz
\end{align*}
for $m\in\{0, 1, 2\}$ in less than a millisecond for a single-core machine. A
method that solves this problem must also provide error guarantees.

A Normal distribution can also be written as
\begin{align*}
\Normal{x}{\mu}{\sigma^2} = \exp\{(x\theta - \theta^2/2)/\phi - (x^2/\phi + \log(2\pi\phi))/2\}
\end{align*}

Let us define
\begin{align*}
h(\theta)= \log(\expfam{y|\outcome, \theta=z} \Normal{z}{\mu}{\sigma^2}) - f_c(y, \theta) +
		\mu^2/(2\sigma^2) + \log(2\pi\sigma^2)/2\\
		=\theta(y/f_a(\phi) + \mu/\sigma^2) - \theta^2/(2\sigma^2) - f_b(\theta)/f_a(\phi)
\end{align*}
and derive


\begin{align*}
h'(\theta) = y/f_a(\phi) + \mu/\sigma^2 - \theta/\sigma^2 - f'_b(\theta)/f_a(\phi)\\
h''(\theta) = -1/\sigma^2 - f''_b(\theta)/f_a(\phi)
\end{align*}
Its second-order Taylor expansion, after some simplification, is
\begin{align*}
\tilde h(\theta) = -f_b(\theta_0)/f_a(\phi) + (f'_b(\theta_0)/f_a(\phi))\theta_0 - (f''_b(\theta_0)/f_a(\phi))\theta_0^2/2\\
	+\theta (y/f_a(\phi) + \eta - f'_b(\theta_0)/a(\phi) + \theta_0 f''_b(\theta_0)/f_a(\phi))\\
	+\theta^2(-\tau - f''_b(\theta_0))/2
\end{align*}
